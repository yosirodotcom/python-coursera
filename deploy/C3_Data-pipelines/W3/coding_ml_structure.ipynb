{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import feature_column\n",
    "\n",
    "from os import getcwd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Numpy File (.npz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the numpy dataset\n",
    "DATA_URL = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\"\n",
    "path = tf.keras.utils.get_file(\"mnist.npz\", DATA_URL)\n",
    "\n",
    "# Extract train, test sets\n",
    "with np.load(path) as data:\n",
    "    train_examples = data[\"x_train\"]\n",
    "    train_labels = data[\"y_train\"]\n",
    "    test_examples = data[\"x_test\"]\n",
    "    test_labels = data[\"y_test\"]\n",
    "    \n",
    "# Load them with tf.data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels))\n",
    "\n",
    "# Apply transformations like batch, shuffle to the dataset\n",
    "train_dataset = train_dataset.shuffle(100).batch(64)\n",
    "test_dataset = test_dataset.batch(64)\n",
    "\n",
    "X, y = next(iter(train_dataset))\n",
    "input_shape = X.numpy().shape[1:]\n",
    "\n",
    "# Create a simple sequential model comprising of a Dense layer\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "    ...\n",
    "    tf.keras.layers.Dense(10, activation='softmax')    \n",
    "])\n",
    "model.compile(...)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cara ke-1, using pandas\n",
    "\n",
    "csv_file = tf.keras.utils.get_file(\n",
    "    \"heart.csv\", \n",
    "    \"https://storage.googleapis.com/applied-dl/heart.csv\",\n",
    ")\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "df[\"thal\"] = pd.Categorical(df[\"thal\"])\n",
    "df[\"thal\"] = df.thal.cat.codes\n",
    "\n",
    "target = df.pop(\"target\")\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((df.values, target.values))\n",
    "train_dataset = dataset.shuffle(len(df)).batch(32)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.git(train_dataset, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cara ke-2\n",
    "csv_file = tf.keras.utils.get_file(\n",
    "    \"heart.csv\", \n",
    "    \"https://storage.googleapis.com/applied-dl/heart.csv\",\n",
    ")\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "target = df.pop(\"target\")\n",
    "\n",
    "dict_slices = tf.data.Dataset.from_tensor_slices((df.to_dict('list'), target.values)).batch(16)\n",
    "\n",
    "for features, target in tfds.as_numpy(dict_slices.take(1)):\n",
    "    for (feature, value), label in zip(features.items(), target):\n",
    "        print(\"{} = {}\\t Label = {}\".format(feature, value, label))\n",
    "        \n",
    "# Constructing the inputs for all the dense features\n",
    "inputs = {key: tf.keras.layers.Input(shape=(), name=key) for key in df.keys()}\n",
    "x = tf.stack(list(inputs.values()), axis=1)\n",
    "x = tf.keras.layers.Dense(10, activation='relu')(x)\n",
    "\n",
    "# The single output denoting the target's probability\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(dict_slices, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cara ke-3\n",
    "\n",
    "# Loading CSV\n",
    "train_file_path = tf.keras.utils.get_file(\n",
    "    \"train.csv\",\n",
    "    \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
    ")\n",
    "test_file_path = tf.keras.utils.get_file(\n",
    "    \"Eval.csv\",\n",
    "    \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
    ")\n",
    "\n",
    "# Extract the data\n",
    "def get_dataset(file_path, **kwargs):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_path,\n",
    "        batch_size=5 # Artificially small to make examples easier to show\n",
    "        label_name=\"survived\",\n",
    "        na_value=\"?\",\n",
    "        num_epochs=1,\n",
    "        ignore_errors=True,\n",
    "        **kwargs\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "raw_train_data = get_dataset(train_file_path)\n",
    "raw_test_data = get_dataset(test_file_path)\n",
    "\n",
    "# Shows the feature key and number of samples for that key. \n",
    "def show_batch(dataset):\n",
    "    for batch, label in dataset.take(1):\n",
    "        for key, value in batch.items():\n",
    "            print(\"{:20s}: {}\".format(key, value.numpy()))\n",
    "show_batch(get_dataset(train_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data from named columns\n",
    "## cara ke-1\n",
    "CSV_COLUMNS = [\n",
    "    \"survived\",\n",
    "    \"sex\",\n",
    "    \"age\",\n",
    "    \"n_siblings_spouses\",\n",
    "    \"parch\",\n",
    "    \"fare\",\n",
    "    \"class\",\n",
    "    \"deck\",\n",
    "    \"embark_town\",\n",
    "    \"alone\"\n",
    "]\n",
    "temp_dataset = get_dataset(train_file_path, column_names=CSV_COLUMNS)\n",
    "\n",
    "show_batch(temp_dataset)\n",
    "\n",
    "## cara ke-2\n",
    "SELECT_COLUMNS = [\n",
    "    \"survived\",\n",
    "    \"age\",\n",
    "    \"n_siblings_spouses\",\n",
    "    \"class\",\n",
    "    \"deck\",\n",
    "    \"alone\"\n",
    "]\n",
    "temp_dataset = get_dataset(train_file_path, select_columns=SELECT_COLUMNS)\n",
    "show_batch(temp_dataset)\n",
    "\n",
    "## Cara ke-3 (menentukan type data secara langsung)\n",
    "SELECT_COLUMNS = [\n",
    "    \"survived\",\n",
    "    \"age\",\n",
    "    \"n_sibilings_spouses\",\n",
    "    \"parch\",\n",
    "    \"fare\"\n",
    "]\n",
    "DEFAULTS = [0, 0.0, 0.0, 0.0, 0.0]\n",
    "temp_dataset = get_dataset(\n",
    "    train_file_path,\n",
    "    select_columns=SELECT_COLUMNS,\n",
    "    colomn_defaults=DEFAULTS\n",
    ")\n",
    "\n",
    "def pack(features, labels): # Function that will pack together all the columns\n",
    "    return tf.stack(list(features.values()), axis=-1), label\n",
    "\n",
    "packed_dataset = temp_dataset.map(pack)\n",
    "## end \n",
    "\n",
    "# Packing numeric features\n",
    "\n",
    "NUMERIC_FEATURES = [\n",
    "    \"age\",\n",
    "    \"n_siblings_spouses\",\n",
    "    \"parch\",\n",
    "    \"fare\"\n",
    "]\n",
    "\n",
    "class PackNumericFeatures(object):\n",
    "    def __init__(self, names):\n",
    "        self.names = names\n",
    "        \n",
    "    def __call__(self, features, labels):\n",
    "        numeric_features = [features.pop(name) for name in self.names]\n",
    "        numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_features]\n",
    "        numeric_features = tf.stack(numeric_features, axis=-1)\n",
    "        features[\"numeric\"] = numeric_features\n",
    "        return features, labels\n",
    "\n",
    "packed_train_data = raw_train_data.map(\n",
    "    PackNumericFeatures(NUMERIC_FEATURES)\n",
    ")\n",
    "packed_test_data = raw_test_data.map(\n",
    "    PackNumericFeatures(NUMERIC_FEATURES)\n",
    ")\n",
    "\n",
    "show_batch(packed_train_data)\n",
    "\n",
    "# Normalizing Numeric features\n",
    "\n",
    "NUMERIC_FEATURES = [\n",
    "    \"age\",\n",
    "    \"n_siblings_spouses\",\n",
    "    \"parch\",\n",
    "    \"fare\"\n",
    "]\n",
    "\n",
    "def normalize_numeric_data(data, mean, std):\n",
    "    return (data-mean)/std\n",
    "\n",
    "desc = pd.read_csv(train_file_path)[NUMERIC_FEATURES].describe()\n",
    "\n",
    "MEAN, STD = np.array(desc.T['mean']), np.array(desc.T['std'])\n",
    "\n",
    "normalizer = functools.partial(normalize_numeric_data, mean=MEAN, std=STD)\n",
    "\n",
    "numeric_column = tf.feature_column.numeric_column(\n",
    "    'numeric',\n",
    "    normalizer_fn=normalizer,\n",
    "    shape=[len(NUMERIC_FEATURES)]\n",
    ")\n",
    "\n",
    "# Handle categorical features\n",
    "\n",
    "CATEGORIES = {\n",
    "    \"sex\": [\"male\", \"female\"],\n",
    "    \"class\": [\"First\", \"Second\", \"Third\"],\n",
    "    \"deck\": [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"],\n",
    "    \"embark_town\": [\"Cherbourg\", \"Southhampton\", \"Queenstown\"],\n",
    "    \"alone\": [\"y\", \"n\"]\n",
    "}\n",
    "\n",
    "cat_feature_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"class\",\n",
    "    vocabulary_list=[\"First\", \"Second\", \"Third\"]\n",
    ")\n",
    "\n",
    "categorical_column = tf.feature_column.indicator_column(cat_feature_col)\n",
    "\n",
    "# Training Model\n",
    "\n",
    "dense_features = tf.keras.layers.DenseFeatures(categorical_column+numeric_column)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    dense_features,\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(packed_train_data, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"insert URL here\"\n",
    "data_root_orig = tf.keras.utils.get_file(\n",
    "    origin=DATA_URL,\n",
    "    fname=\"flower_photos\",\n",
    "    untar=True\n",
    ")\n",
    "data_root = pathlib.Path(data_root_orig)\n",
    "\n",
    "# Load all the file paths in the directory\n",
    "all_image_paths = list(data_root.glob('*/*'))\n",
    "all_image_paths = [str(path) for path in all_image_paths]\n",
    "\n",
    "# Gather the list of labels and create a labelmap\n",
    "label_names = sorted(item.name for item in data_root.glob('*/') if item.is_dir())\n",
    "label_to_index = dict((name, index) for index, name in enumerate(label_names))\n",
    "\n",
    "# Use the label map to fetch all categorical labels\n",
    "all_image_labels = [label_to_index[pathlib.Path(path).parent.name] for path in all_image_labels]\n",
    "\n",
    "path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
    "label_ds = tf.data.Dataset.from_tensor_slices(all_image_labels)\n",
    "\n",
    "def preprocess_image(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [192, 192])\n",
    "    image /= 255.0 # normalize to [0,1] range\n",
    "    return image\n",
    "\n",
    "image_ds = path_ds.map(preprocess_image)\n",
    "image_label_ds = tf.data.Dataset.zip((image_ds, label_ds))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "ds = image_label_ds.shuffle(buffer_size=len(all_image_paths)).repeat().batch(BATCH_SIZE)\n",
    "steps_per_epoch = tf.math.ceil(len(all_image_paths) / BATCH_SIZE).numpy()\n",
    "model.fit(ds, epochs=1, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details on choosing an appropriate value for steps_per_epoch (from the \n",
    "tf.keras.Model documentation https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=stable\n",
    ")\n",
    "\n",
    "steps_per_epoch: \n",
    "\n",
    "Integer or None. \n",
    "\n",
    "Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. \n",
    "\n",
    "When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. \n",
    "\n",
    "If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/illiad/\"\n",
    "FILE_NAMES = [\"cowper.txt\", \"derby.txt\", \"butler.txt\"]\n",
    "\n",
    "def labeler(example, index):\n",
    "    return example, tf.cast(index, tf.int64)\n",
    "\n",
    "labeled_data_sets = []\n",
    "for i, file_name in enumerate(FILE_NAMES):\n",
    "    file_path = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+file_name)\n",
    "    lines_dataset = tf.data.TextLineDataset(file_path) # used to load text from a file into a dataset\n",
    "    labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
    "    labeled_data_sets.append(labeled_dataset)\n",
    "\n",
    "# Preparing the dataset\n",
    "\n",
    "dataset = labeled_data_sets[0]\n",
    "for labeled_dataset in labeled_data_sets[1:]:\n",
    "    dataset = dataset.concatenate(labeled_dataset)\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=50000)\n",
    "\n",
    "for ex in dataset.take(5):\n",
    "    print(ex[0].numpy(), ex[1].numpy())\n",
    "    \n",
    "# Text Encoding\n",
    "\n",
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "for text_tensor, _ in all_labeled_data:\n",
    "    some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "    vocabulary_set.update(some_tokens)\n",
    "    \n",
    "vocab_size = len(vocabulary_set)\n",
    "\n",
    "# Encode an example\n",
    "\n",
    "original_text = next(iter(all_labeled_data))[0].numpy() # Show one of the labeled data\n",
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set) # Create an text encoder with a fixed vocabulary set\n",
    "encoded_text = encoder.encode(original_text) # Encode an example\n",
    "\n",
    "# Encode all the examples\n",
    "\n",
    "def encode(text_tensor, label):\n",
    "    encoded_text = encoder.encode(text_tensor.numpy())\n",
    "    return encoded_text, label\n",
    "\n",
    "def encode_map_fn(text, label):\n",
    "    return tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64))\n",
    "\n",
    "all_encoded_data = all_labeled_data.map(encode_map_fn)\n",
    "\n",
    "# Prepare the dataset\n",
    "\n",
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "TAKE_SIZE = 5000\n",
    "\n",
    "train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\n",
    "train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))\n",
    "\n",
    "test_data = all_encoded_data.take(TAKE_SIZE)\n",
    "test_data = test_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))\n",
    "\n",
    "# Training the model\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.Sequential([tf.keras.layers.Dense(units, activation='relu') for units in [64,64]]),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(train_data, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
